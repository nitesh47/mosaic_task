---
version: 0.1

random_state: 42

data:
  val_size: 0.2
  test_size: 0.1

encoding:
  max_length: 448

transformers:
  tokenizer: distilbert-base-uncased
  model: distilbert-base-uncased
  activation: sigmoid

train:
  batch_size: 32
  epochs: 100
  optimization:
    init_lr: 0.00002
  callbacks:
    verbose: true
    early_stopping:
      min_delta: 0
      patience: 2
    reduce_on_plateau:
      factor: 0.2
      patience: 1
      min_lr: 0.000001
inference:
  batch_size: 32
